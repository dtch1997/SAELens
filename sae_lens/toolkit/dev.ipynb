{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.toolkit.sae_attrib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricFunction(Protocol):\n",
    "    def __call__(\n",
    "        self, model: HookedTransformer, text: str\n",
    "    ) -> tuple[Float[torch.Tensor, \"\"], ActivationCache]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def _next_token_loss(\n",
    "    model: HookedTransformer, text: str\n",
    ") -> tuple[Float[torch.Tensor, \"\"], ActivationCache]:\n",
    "    \"\"\"Compute the next token loss\"\"\"\n",
    "    loss, cache = model.run_with_cache(text, return_type=\"loss\")\n",
    "    assert isinstance(loss, torch.Tensor)\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "def get_sae_features_and_errors_for_input(\n",
    "    model: HookedTransformer,\n",
    "    sae_dict: SparseAutoencoderDictionary,\n",
    "    metric_fn: MetricFunction,\n",
    "    x: str,\n",
    ") -> tuple[\n",
    "    dict[str, Float[torch.Tensor, \"n_batch n_token d_sae\"]],\n",
    "    dict[str, Float[torch.Tensor, \"n_batch n_token d_model\"]],\n",
    "]:\n",
    "    # Run model on original input\n",
    "    patchers = {name: SAEPatcher(sae) for name, sae in sae_dict}\n",
    "    with model.hooks(\n",
    "        fwd_hooks=[p.get_forward_hook() for p in patchers.values()],\n",
    "        bwd_hooks=[p.get_backward_hook() for p in patchers.values()],\n",
    "    ):\n",
    "        metric, _ = metric_fn(model, x)\n",
    "        metric.backward()\n",
    "\n",
    "    # Collate the sae features and errors\n",
    "    orig_features = {\n",
    "        name: patcher.sae_feature_acts for name, patcher in patchers.items()\n",
    "    }\n",
    "    orig_errors = {name: patcher.sae_errors for name, patcher in patchers.items()}\n",
    "    return orig_features, orig_errors\n",
    "\n",
    "\n",
    "def compute_node_indirect_effect(\n",
    "    model: HookedTransformer,\n",
    "    sae_dict: SparseAutoencoderDictionary,\n",
    "    metric_fn: MetricFunction,\n",
    "    x_orig: str,\n",
    "    x_patch: str | None = None,\n",
    ") -> dict[str, Float[torch.Tensor, \"n_batch n_token (d_sae + d_model)\"]]:\n",
    "    \"\"\"Compute node indirect effects for a given input\n",
    "\n",
    "    Here, nodes are the features and errors of the SAEs\n",
    "    and the indirect effect is computed using first-order Taylor approximation.\n",
    "\n",
    "    Returns:\n",
    "    - A dict of {name: [batch, token, d_sae + d_model]} of node scores\n",
    "        NOTE: the first d_sae elements are features, the rest are errors\n",
    "    \"\"\"\n",
    "    orig_features, orig_errors = get_sae_features_and_errors_for_input(\n",
    "        model, sae_dict, metric_fn, x_orig\n",
    "    )\n",
    "\n",
    "    # If no patch is provided, return zeros\n",
    "    if x_patch is None:\n",
    "        patch_features = {\n",
    "            name: torch.zeros_like(act) for name, act in orig_features.items()\n",
    "        }\n",
    "        patch_errors = {\n",
    "            name: torch.zeros_like(err) for name, err in orig_errors.items()\n",
    "        }\n",
    "    else:\n",
    "        patch_features, patch_errors = get_sae_features_and_errors_for_input(\n",
    "            model, sae_dict, metric_fn, x_patch\n",
    "        )\n",
    "\n",
    "    # Compute feature scores\n",
    "    feature_scores = {}\n",
    "    for name, orig_act in orig_features.items():\n",
    "        grad_m_orig_act = orig_act.grad\n",
    "        assert grad_m_orig_act is not None\n",
    "        patch_act = patch_features[name]\n",
    "        feature_scores[name] = indirect_effect_attrib(\n",
    "            orig_act, patch_act, grad_m_orig_act\n",
    "        )\n",
    "\n",
    "    # Compute error scores\n",
    "    error_scores = {}\n",
    "    for name, orig_err in orig_errors.items():\n",
    "        grad_m_orig_err = orig_err.grad\n",
    "        assert grad_m_orig_err is not None\n",
    "        patch_err = patch_errors[name]\n",
    "        error_scores[name] = indirect_effect_attrib(\n",
    "            orig_err, patch_err, grad_m_orig_err\n",
    "        )\n",
    "\n",
    "    # Combine feature and error scores\n",
    "    node_scores = {}\n",
    "    for name in sae_dict:\n",
    "        node_scores[name] = torch.cat(\n",
    "            [feature_scores[name], error_scores[name]], dim=-1\n",
    "        )\n",
    "    return node_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:06<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "\n",
    "from sae_lens.training.sae_group import SparseAutoencoderDictionary\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2\").to(device)\n",
    "autoencoders: dict[str, SparseAutoencoder] = get_gpt2_res_jb_saes()[0]\n",
    "sae_dict = SparseAutoencoderDictionary(list(autoencoders.values())[0].cfg)\n",
    "sae_dict.autoencoders = autoencoders\n",
    "sae_dict.to(device)\n",
    "prompt = \"Hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('blocks.0.hook_resid_pre', SparseAutoencoder(\n  (hook_sae_in): HookPoint()\n  (hook_hidden_pre): HookPoint()\n  (hook_hidden_post): HookPoint()\n  (hook_sae_out): HookPoint()\n))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m node_ies \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_node_indirect_effect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_next_token_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m, in \u001b[0;36mcompute_node_indirect_effect\u001b[0;34m(model, sae_dict, metric_fn, x_orig, x_patch)\u001b[0m\n\u001b[1;32m     97\u001b[0m node_scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m sae_dict:\n\u001b[1;32m     99\u001b[0m     node_scores[name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m--> 100\u001b[0m         [\u001b[43mfeature_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m, error_scores[name]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node_scores\n",
      "\u001b[0;31mKeyError\u001b[0m: ('blocks.0.hook_resid_pre', SparseAutoencoder(\n  (hook_sae_in): HookPoint()\n  (hook_hidden_pre): HookPoint()\n  (hook_hidden_post): HookPoint()\n  (hook_sae_out): HookPoint()\n))"
     ]
    }
   ],
   "source": [
    "node_ies = compute_node_indirect_effect(model, sae_dict, _next_token_loss, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(\n",
    "    upstream: Float[torch.Tensor, \"n_batch n_token dim_u\"],\n",
    "    downstream: Float[torch.Tensor, \"n_batch n_token dim_d\"],\n",
    ") -> Float[torch.Tensor, \"n_batch n_token dim_u dim_d\"]:\n",
    "    \"\"\"Compute the Jacobian of downstream w.r.t upstream\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def compute_edge_indirect_effect(\n",
    "    model: HookedTransformer,\n",
    "    sae_dict: SparseAutoencoderDictionary,\n",
    "    metric_fn: Callable[[HookedTransformer, str], Float[torch.Tensor, \"\"]],\n",
    "    x_orig: str,\n",
    "    x_patch: str | None = None,\n",
    ") -> tuple[\n",
    "    dict[str, Float[torch.Tensor, \"n_batch n_token d_sae\"]],\n",
    "    dict[str, Float[torch.Tensor, \"n_batch n_token d_model\"]],\n",
    "]:\n",
    "    pass\n",
    "\n",
    "    return {}, {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
